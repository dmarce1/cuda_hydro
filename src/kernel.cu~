#include <limits>
#include <future>
#include "kernel.hpp"

#define NRK 2
#define CFL 0.4
#define DE_SWITCH_1 double(0.001)
#define DE_SWITCH_2 double(0.01)
#define THETA 1.3

#define INVOKE2( func, blocks, threads, ...) \
	func <<< blocks, threads >>> ( __VA_ARGS__ ); \
	cudaStreamSynchronize(0)

#define INVOKE3( func, blocks, threads, size, ...) \
	func <<< blocks, threads, size >>> ( __VA_ARGS__ ); \
	cudaStreamSynchronize(0)

__device__
inline double minmod(double a, double b) {
	return (copysign(0.5, a) + copysign(0.5, b)) * fmin(fabs(a), fabs(b));
}

__device__
inline double minmod_theta(double a, double b) {
	return minmod(double(THETA) * minmod(a, b), double(0.5) * (a + b));
}

void cuda_exit() {
	cudaDeviceReset();
}

__global__
void cuda_prep(double* U_base, double* U0_base, double* dU_base) {
	const int nx = blockDim.x;
	const int ny = gridDim.x;
	const int nz = gridDim.y;
	const int xi = threadIdx.x;
	const int yi = blockIdx.x;
	const int zi = blockIdx.y;
	const int nx1 = nx + 2 * BW;
	const int ny1 = ny + 2 * BW;
	const int nz1 = nz + 2 * BW;
	const int sz = nx1 * ny1 * nz1;
	const int idx = (xi + BW) + nx1 * (yi + BW) + (nx1 * ny1) * (zi + BW);
	double* U[NF];
	double* U0[NF];
	double* dU[NF];
	for (int f = 0; f != NF; ++f) {
		U[f] = U_base + f * sz;
		U0[f] = U0_base + f * sz;
		dU[f] = dU_base + f * sz;
	}
	for (int f = 0; f != NF; ++f) {
		dU[f][idx] = 0.0;
		U0[f][idx] = U[f][idx];
	}

}

__global__
void cuda_flux(double* U_base, double* dU_base, double dx, int dim, int di,
		double* avisc) {
	int nx, ny, nz;
	int xi, yi, zi;
	int nx1, ny1, nz1;

	__shared__
	extern double shared_real[];

	switch (dim) {
	case XDIM:
		nx = blockDim.x;
		ny = gridDim.x;
		nz = gridDim.y;
		xi = threadIdx.x;
		yi = blockIdx.x;
		zi = blockIdx.y;
		nx1 = nx + 2 * BW - 1;
		ny1 = ny + 2 * BW;
		nz1 = nz + 2 * BW;
		break;
	case YDIM:
		ny = blockDim.x;
		nx = gridDim.x;
		nz = gridDim.y;
		yi = threadIdx.x;
		xi = blockIdx.x;
		zi = blockIdx.y;
		nx1 = nx + 2 * BW;
		ny1 = ny + 2 * BW - 1;
		nz1 = nz + 2 * BW;
		break;
		/*case ZDIM:*/
	default:
		nz = blockDim.x;
		nx = gridDim.x;
		ny = gridDim.y;
		zi = threadIdx.x;
		xi = blockIdx.x;
		yi = blockIdx.y;
		nx1 = nx + 2 * BW;
		ny1 = ny + 2 * BW;
		nz1 = nz + 2 * BW - 1;
	}

	const int idx = (xi + BW) + nx1 * (yi + BW) + (nx1 * ny1) * (zi + BW);
	const int sz = (nx1) * (ny1) * (nz1);
	double F[NF];
	double* U[NF];
	double* dU[NF];
	for (int f = 0; f != NF; ++f) {
		U[f] = U_base + f * sz;
		dU[f] = dU_base + f * sz;
	}
	const int mom_dim = mom_i + dim;
	if (threadIdx.x == 0) {
		for (int f = 0; f != NF; ++f) {
			U[f][idx - 2 * di] = U[f][idx - di] = U[f][idx];
		}
		U[mom_dim][idx - 2 * di] = U[mom_dim][idx - di] = fmax(0.0,
				U[mom_dim][idx]);
	} else if (threadIdx.x == blockDim.x - 2) {
		for (int f = 0; f != NF; ++f) {
			U[f][idx + 2 * di] = U[f][idx + di] = U[f][idx];
		}
		U[mom_dim][idx + 2 * di] = U[mom_dim][idx + di] = fmin(0.0,
				U[mom_dim][idx]);
	}
	__syncthreads();

	double dxinv = double(1.0) / dx;
	double slm, slp;
	double vm2[NF], vm1[NF], vp1[NF], vp2[NF];
	double UR[NF], UL[NF];
	double ar, al;
	double pl, pr;
	double vl, vr;
	double cl, cr;
	double ekr, ekl;
	double eir, eil;
	double a;
	const int im2 = idx - 2 * di;
	const int im1 = idx - di;
	const int ip1 = idx;
	const int ip2 = idx + di;
	vm2[den_i] = U[den_i][im2];
	vm1[den_i] = U[den_i][im1];
	vp1[den_i] = U[den_i][ip1];
	vp2[den_i] = U[den_i][ip2];
	const double rho_m2_inv = 1.0 / U[den_i][im2];
	const double rho_m1_inv = 1.0 / U[den_i][im1];
	const double rho_p1_inv = 1.0 / U[den_i][ip1];
	const double rho_p2_inv = 1.0 / U[den_i][ip2];
	for (int f = 1; f < NF; ++f) {
		vm2[f] = U[f][im2] * rho_m2_inv;
		vm1[f] = U[f][im1] * rho_m1_inv;
		vp1[f] = U[f][ip1] * rho_p1_inv;
		vp2[f] = U[f][ip2] * rho_p2_inv;
	}
	for (int f = 0; f != NF; ++f) {
		slm = minmod_theta(vm1[f] - vm2[f], vp1[f] - vm1[f]);
		slp = minmod_theta(vp1[f] - vm1[f], vp2[f] - vp1[f]);
		UL[f] = vm1[f] + 0.5 * slm;
		UR[f] = vp1[f] - 0.5 * slp;
	}
	for (int f = 1; f < NF; ++f) {
		UL[f] *= UL[den_i];
		UR[f] *= UR[den_i];
	}
	ekr = ekl = 0.0;
	for (int d = 0; d != NDIM; ++d) {
		ekl += UL[mom_i + d] * UL[mom_i + d];
		ekr += UR[mom_i + d] * UR[mom_i + d];
	}
	const double rhoLinv = double(1.0) / UL[den_i];
	const double rhoRinv = double(1.0) / UR[den_i];
	ekl *= 0.5 * rhoLinv;
	ekr *= 0.5 * rhoRinv;
	const double etl = UL[ene_i];
	const double etr = UR[ene_i];
	eil = etl - ekl;
	eir = etr - ekr;
	if (eil < etl * DE_SWITCH_1) {
		eil = pow(UL[tau_i], FGAMMA);
	}
	if (eir < etr * DE_SWITCH_1) {
		eir = pow(UR[tau_i], FGAMMA);
	}
	pl = (FGAMMA - 1.0) * eil;
	pr = (FGAMMA - 1.0) * eir;
	cl = sqrt(FGAMMA * pl * rhoLinv);
	cr = sqrt(FGAMMA * pr * rhoRinv);
	vl = UL[mom_dim] * rhoLinv;
	vr = UR[mom_dim] * rhoRinv;
	al = fabs(vl) + cl;
	ar = fabs(vr) + cr;
	a = fmax(al, ar);
	for (int f = 0; f != NF; ++f) {
		F[f] = vr * UR[f] + vl * UL[f] - a * (UR[f] - UL[f]);
	}
	F[mom_dim] += pl + pr;
	F[ene_i] += vl * pl + vr * pr;
	for (int f = 0; f != NF; ++f) {
		F[f] *= 0.5;
	}
	const bool even = threadIdx.x % 2 == 0;
	if( even ) {
		for( int f = 0; f != NF; ++f ) {
			dU[f][idx] += F[f] * dxinv;
		}
		__syncthreads();
		for( int f = 0; f != NF; ++f ) {
			dU[f][idx] += F[f] * dxinv;
		}
	} else {
		for( int f = 0; f != NF; ++f ) {
			dU[f][idx] += F[f] * dxinv;
		}
		__syncthreads();
		for( int f = 0; f != NF; ++f ) {
			dU[f][idx] += F[f] * dxinv;
		}
	}

	if (avisc != nullptr) {
		int tid = threadIdx.x
				+ blockDim.x * (blockIdx.x + gridDim.x * blockIdx.y);
		avisc[tid] = a;
	}

}

__global__
void cuda_advance(double* U_base, double* U0_base, double* dU_base, double dt,
		double beta) {
	const int nx1 = blockDim.x + 2 * BW;
	const int ny1 = gridDim.x + 2 * BW;
	const int nz1 = gridDim.y + 2 * BW;
	const int xi = threadIdx.x;
	const int yi = blockIdx.x;
	const int zi = blockIdx.y;
	const int idx = (xi + BW) + nx1 * (yi + BW) + (nx1 * ny1) * (zi + BW);
	const int sz = (nx1) * (ny1) * (nz1);
	double* U[NF];
	double* U0[NF];
	double* dU[NF];
	for (int f = 0; f != NF; ++f) {
		U[f] = U_base + f * sz;
		U0[f] = U0_base + f * sz;
		dU[f] = dU_base + f * sz;
	}
	for (int f = 0; f != NF; ++f) {
		U[f][idx] += beta * dt * dU[f][idx]
				+ (U0[f][idx] - U[f][idx]) * (1.0 - beta);
		dU[f][idx] = 0.0;
	}
	double ek = 0.0;
	const double rhoinv = double(1.0) / U[den_i][idx];
	for (int dim = 0; dim != NDIM; ++dim) {
		ek += U[mom_i + dim][idx] * U[mom_i + dim][idx];
	}
	ek *= 0.5 * rhoinv;
	const double etot = U[ene_i][idx];
	const double ei = etot - ek;
	if (ei > etot * DE_SWITCH_2) {
		U[tau_i][idx] = pow(ei, 1.0 / FGAMMA);
	}
}

double cuda_hydro_wrapper(double* rho, double* s[NDIM], double* egas, int nx,
		int ny, int nz, double dx) {
	static bool first_call = true;
	const int sz = nx * ny * nz;

	static double* U;
	static double* U0;
	static double* dU;
	static double* avisc[NDIM];
	static double* local_avisc[NDIM];

	static dim3 blocks[NDIM];
	static dim3 threads[NDIM];
	static dim3 blocks0(ny - 2 * BW, nz - 2 * BW);
	static dim3 threads0(nx - 2 * BW);

	double dt;

	if (first_call) {
		blocks[XDIM] = dim3(ny - 2 * BW, nz - 2 * BW);
		blocks[YDIM] = dim3(nx - 2 * BW, nz - 2 * BW);
		blocks[ZDIM] = dim3(nx - 2 * BW, ny - 2 * BW);
		threads[XDIM] = dim3(nx - 2 * BW + 1);
		threads[YDIM] = dim3(ny - 2 * BW + 1);
		threads[ZDIM] = dim3(nz - 2 * BW + 1);
		for (int dim = 0; dim != NDIM; ++dim) {
			const int this_sz = blocks[dim].x * blocks[dim].y * threads[dim].x;
			cudaMalloc(&(avisc[dim]), this_sz * sizeof(double));
			local_avisc[dim] = new double[this_sz];
		}
		cudaMalloc(&U, NF * sz * sizeof(double));
		cudaMalloc(&U0, NF * sz * sizeof(double));
		cudaMalloc(&dU, NF * sz * sizeof(double));
		first_call = false;
	}

	cudaMemcpy(U + sz * den_i, rho, sz * sizeof(double),
			cudaMemcpyHostToDevice);
	cudaMemcpy(U + sz * ene_i, egas, sz * sizeof(double),
			cudaMemcpyHostToDevice);
	for (int d = 0; d != NDIM; ++d) {
		cudaMemcpy(U + sz * (mom_i + d), s[d], sz * sizeof(double),
				cudaMemcpyHostToDevice);
	}

	const int di[NDIM] = { 1, nx, nx * ny };

	INVOKE2(cuda_prep, blocks0, threads0, U, U0, dU);

	std::future<double> dt_fut[NDIM];

	dt = std::numeric_limits<double>::max();

	for (int rk = 0; rk < NRK; ++rk) {
		const double beta = rk == 0 ? 1.0 : 0.5;

		for (int dim = 0; dim != NDIM; ++dim) {
			dt_fut[dim] =
					std::async(std::launch::async,
							[=]() {
								INVOKE2(cuda_flux, (blocks[dim]),(threads[dim]),U, dU, dx, dim, (di[dim]), (rk == 0 ? avisc[dim] : nullptr));
								double dt = std::numeric_limits<double>::max();
								if (rk == 0) {
									const int this_sz = blocks[dim].x * blocks[dim].y * threads[dim].x;
									cudaMemcpy(local_avisc[dim], avisc[dim], this_sz * sizeof(double),
											cudaMemcpyDeviceToHost);
									for (int b = 0; b < this_sz; ++b) {
										dt = std::min(dt, CFL * dx / local_avisc[dim][b]);
									}
								}
								return dt;
							});
		}

		for (int dim = 0; dim != NDIM; ++dim) {
			const double this_dt = dt_fut[dim].get();
			if (rk == 0) {
				dt = std::min(dt, this_dt);
			}
		}
		/**/INVOKE2(cuda_advance, blocks0, threads0, U, U0, dU, dt, beta);

		/**/}

	cudaMemcpy(rho, U + sz * den_i, sz * sizeof(double),
			cudaMemcpyDeviceToHost);
	cudaMemcpy(egas, U + sz * ene_i, sz * sizeof(double),
			cudaMemcpyDeviceToHost);
	for (int d = 0; d != NDIM; ++d) {
		cudaMemcpy(s[d], U + sz * (mom_i + d), sz * sizeof(double),
				cudaMemcpyDeviceToHost);
	}

	return dt;
}
